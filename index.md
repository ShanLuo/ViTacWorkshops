---
title: Home
layout: default
---

# ICRA 2024 Workshop on “Robot Embodiment through Visuo-Tactile Perception”

This is the official website for "ViTac 2024: Robot Embodiment through Visuo-Tactile Perception".

Abstract: This is the 5th time for us to organise the ViTac workshop in the ICRA conferences, after ICRA 2019, 2020, 2021 and 2023 ViTac Workshops. It will also be timely for us to report the progress of the Special Issue (now called Special Collection) we are organising on Tactile Robotics at the IEEE Transactions on Robotics. The recent trend in embodied intelligence, i.e., intelligence that requires and leverages a physical body of a robot, demands rich physical interactions with the surrounding environment, which cannot be achieved without perception of the environment with vision and tactile sensing. Recent years have seen rapid advancements in tactile sensor development, integration of information from visual and tactile modalities, leveraging vision and tactile sensing in agile grasping and manipulation, and simulations that involve both vision and tactile sensing. It will be timely to bring together the experts and young researchers in the field to discuss these important topics in robotics. This proposed full-day workshop will cover the recent advancements in the area of visuo-tactile sensing and perception, with the aim to empower robots with visual and tactile intelligence. It will further enhance active collaboration and address challenges for this important topic and applications.

In our one-day workshop, we will discuss topics relating to visuo-tactile perception, from the development of tactile sensors, to integration of vision and touch for robot physical interaction tasks, to the creation of simulations of tactile sensors, and Sim2Real learning for visuo-tactile perception. The workshop will bring together experts from a diverse range of disciplines and encompass engineers, computer scientists, cognitive scientists and sensor developers to discuss the important topic. We will report the progress of the Special Issue (now called Special Collection) we are organising on Tactile Robotics at the IEEE Transactions on Robotics and invite the authors of the accepted papers in our special issue to present their works in our workshop. As part of the workshop, we will also organise a Challenge named [ManiSkill-ViTac](https://ai-workshops.github.io/maniskill-vitac-challenge-2024/) to benchmark the use of vision and tactile sensing in robot manipulation tasks. 

For past ViTac workshops, please check [ViTac2023](https://shanluo.github.io/ViTacWorkshops/vitac2023), [ViTac2021](https://shanluo.github.io/ViTacWorkshops/vitac2021), [ViTac2020](https://shanluo.github.io/ViTacWorkshops/vitac2020), [ViTac2019](https://shanluo.github.io/ViTacWorkshops/vitac2019).

**Time:** Full day, 17th May 2024 <br>
<span style="color:blue"> **Location:** G411 (North area), PACIFICO Yokohama (Hybrid), Yokohama, Japan. </span> <br>

{% include figure.html img="vitac2024_flyer.jpg" alt="intro image here" caption="ICRA 2024 ViTac worshop" width="75%" %}

**(Tentative) Program** <br>
**09:00-09:10**	Organisers - Shan Luo:	Welcome & updates on TRO Special Collection <br>

**Session 1: Development of touch sensors - Hardware intelligence**		<br>
**09:10-09:20**	Nathan Lepora & Shan Luo:	Session Intro <br>
**09:20-09:40**	Ravinder Dahiya:	“Movement Detection using Self-Powered e-Skin” <br>
**09:40-10:00**	Antonio Bicchi:	"From optic flow to tactile flow. Background and new results" <br>
**10:00-10:30**	Coffee break, posters and demos	<br>

**Session 2: Robot embodiment with vision and tactile sensing**		<br>
**10:30-10:40**	Wenzhen Yuan & Shan Luo:	Session Intro <br>
**10:40-11:00**	Huaping Liu:	"Large-scale tactile perception for interaction and manipulation" <br>
**11:00-11:20**	Van Anh Ho:	"How to make robot “feel" safe?: Embodiment in robot safety with vision-based multimodal sensing" <br>
**11:20-12:00**	Poster presenters	Lightning poster presentations (2mins each) <br>
**12:00-14:00**	Lunch break, posters and demos	<br>

**Session 3: Challenges in visual-tactile perception**		<br>
**14:00-14:10**	Rui Chen & Hao Su	Session: intro <br>
**14:10-14:30**	Mark Cutkosky:	Invited talk <br>
**14:30-14:50**	Roberto Calandra:	"Towards General In-hand Manipulation with Touch" <br>
**14:50-15:10**	ManiSkill-ViTac Challenge	Conclusion of ManiSkill-ViTac Challenge, Award Ceremony <br>
**15:10-15:30**	ManiSkill-ViTac Challenge	Invited talk from challenge winners <br>
**15:30-16:00**	Coffee break, posters and demos	<br>

**Session 4 The future of robot embodiment with visual-tactile perception**		<br>
**16:00-16:10**	Kaspar Althoefer & Gordon Cheng:	Session intro <br>
**16:10-16:30**	Perla Maiolino:	"Proxy-tactile technology and tactile data generation for robot self-awareness and control" <br>
**16:30-16:50**	Maria Bauza Villalonga:	"Visuo-tactile sensing for precise robotic manipulation with multiple embodiments" <br>
**16:50-17:00**	Room rearrangment for the group discussion <br>
**17:00-17:30**	All speakers:	Group discussion <br>
**17:30**	Finish	<br>

**Accepted papers** <br>
<ol>
  <li><a href="content/ViTac2024_Paper_01.pdf">Zeqing Zhang, Guangze Zheng, Xuebo Ji, Guanqi Chen, Ruixing Jia, Wentao Chen, Guanhua Chen, Liangjun Zhang and Jia Pan. "MAE4GM: Visuo-Tactile Learning for Property Estimation of Granular Material using Multimodal Autoencoder" </a></li>
  <li><a href="content/ViTac2024_Paper_02.pdf">Emanuele Aucone, Carmelo Sferrazza, Manuel Gregor, Raffaello D’Andrea, and Stefano Mintchev. "Optical Tactile Sensing for Multi-Contact Interaction on Aerial Robots" </a></li>
  <li><a href="content/ViTac2024_Paper_03.pdf">Osher Azulay and Avishai Sintov. "AllSight: Advancing Optical Tactile Sensing and Sim-to-Real Learning for Dexterous Robotic Manipulation" </a></li>
  <li><a href="content/ViTac2024_Paper_04.pdf">Julia Di, Zdravko Dugonjic, Will Fu, Tingfan Wu, Romeo Mercado, Kevin Sawyer, Victoria Rose Most, Gregg Kammerer, Stefanie Speidel, Richard E. Fan, Geoffrey Sonn, Mark R. Cutkosky, Mike Lambeta, and Roberto Calandra. "DIGIT Pinki: Using Fiber Optic Bundles to Miniaturize Vision-Based Tactile Sensors" </a></li>
  <li><a href="content/ViTac2024_Paper_05.pdf">Yijiong Lin, Alex Church, Max Yang, Haoran Li, John Lloyd, Dandan Zhang, Nathan F. Lepora. "Bi-Touch: Bimanual Tactile Manipulation with Sim-to-Real Deep Reinforcement Learning" </a></li>
  <li><a href="content/ViTac2024_Paper_06.pdf">Hao Zhou, Masahiro Miyazaki and Kazuhiro Shimonomura. "NailTact: Vision-based Tactile Sensing in Both Fingerpad and Nail" </a></li>
  <li><a href="content/ViTac2024_Paper_07.pdf">Chenghua Lu, Nathan F. Lepora. "DexiTac: A Reconfigurable Gripper with Tactile Sensing Ability" </a></li>
  <li><a href="content/ViTac2024_Paper_08.pdf">Giuseppe Vitrani and Michae ̈l Wiertlewski. "Predicting object slippage in robotic grippers using human-inspired tactile processing" </a></li>
  <li><a href="content/ViTac2024_Paper_09.pdf">Boyi Duan, Kun Qian, Yongqiang Zhao, Dongyuan Zhang, Shan Luo. "Feature-level Sim2Real Regression of Tactile Images for Robot Manipulation" </a></li>
  <li><a href="content/ViTac2024_Paper_10.pdf">Haoran Li, Saekwang Nam, Zhenyu Lu, Chenguang Yang, Efi Psomopoulou, Nathan F. Lepora. "BioTacTip: A Soft Biomimetic Optical Tactile Sensor for Efficient 3D Contact Localization and 3D Force Estimation" </a></li>
  <li><a href="content/ViTac2024_Paper_12.pdf">Bo Ai, Stephen Tian, Haochen Shi, Yixuan Wang, Cheston Tan, Yunzhu Li, Jiajun Wu. "RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing" </a></li>
  <li><a href="content/ViTac2024_Paper_13.pdf">Max Yang, Chenghua Lu, Alex Church, Yijiong Lin, Chris Ford, Haoran Li, Efi Psomopoulou, David A.W. Barton, Nathan F. Lepora. "AnyRotate: Gravity-Invariant In-Hand Rotation with Sim-to-Real Touch" </a></li>
  <li><a href="content/ViTac2024_Paper_14.pdf">Quan Khanh Luu, and Van Anh Ho. "Soft Robotic Link with Controllable Transparency for Vision-based Tactile and Proximity Sensing" </a></li>
  <li><a href="content/ViTac2024_Paper_16.pdf">Takeshi Tomomizu, Quan Khanh Luu, Nhan Huu Nguyen, and Van Anh Ho. "Preliminary Design of Vision-Based Tactile Sensor with Nail and Soft Structures" </a></li>
  <li><a href="content/ViTac2024_Paper_17.pdf">Zhuo Chen, Ni Ou, Jiaqi Jiang and Shan Luo. "Deep Domain Adaptation Regression for Force Calibration of Optical Tactile Sensors" </a></li>
  <li><a href="content/ViTac2024_Paper_18.pdf">Yuni Fuchioka and Masashi Hamaya. "In-Grasp Torque Estimation for Visuotactile Sensors with Tactile Dipole Moments" </a></li>
  <li><a href="content/ViTac2024_Paper_19.pdf">Xuyang Zhang, Jiaqi Jiang, and Shan Luo. "RoTip: A Finger-Shaped Tactile Sensor with Active Rotation" </a></li>
  <li><a href="content/ViTac2024_Paper_20.pdf">Tunwu Li, Xiaolong Li, Zhenyu Lu and Chenguang Yang. "A Tactile Sensor Roller for In-Process Inspection of Composites" </a></li>
  <li><a href="content/ViTac2024_Paper_21.pdf">Jieji Ren, Yueshi Dong, Yuru Gong, Ningbin Zhang, Jiang Zou and Guoying Gu. "Soft Camera-based Tactile Sensor for Compliant Grasping and Manipulation" </a></li>
  <li><a href="content/ViTac2024_Paper_22.pdf">Abdallah Ayad, Adrian Ro ̈fer, Nick Heppert, Abhinav Valada. "Imagine2touch: Predictive Tactile Sensing for Robotic Manipulation using Efficient Low-Dimensional Signals" </a></li>
  <li><a href="content/ViTac2024_Paper_23.pdf">Zhongyue Wu and Maciej Wozniak. "Enhanced AR: Integrating Haptic Feedback from Tactile Sensors for Immersive Teleportation" </a></li>
  <li><a href="content/ViTac2024_Paper_24.pdf">Lowiek Van den Stockt, Remko Proesmans and Francis wyffels. "Automatic Calibration for an Open-source Magnetic Tactile Sensor" </a></li>
  <li><a href="content/ViTac2024_Paper_25.pdf">Noah Becker, Erik Gattung, Kay Hansel, Tim Schneider, Yaonan Zhu, Yasuhisa Hasegawa, and Jan Peters. "Integrating Visuo-tactile Sensing with Haptic Feedback for Teleoperated Robot Manipulation" </a></li>
  <li><a href="content/ViTac2024_Paper_26.pdf">Eric T. Chang, Peter Ballentine, Ioannis Kymissis and Matei Ciocarlie. "Development Towards a PVDF-Based Tactile Finger with Distributed Vibration Sensing" </a></li>
  <li><a href="content/ViTac2024_Paper_27.pdf">Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent Yi, Sergey Levine, and Jitendra Malik. "Learning Visuotactile Skills with Two Multifingered Hands" </a></li>
  <li><a href="content/ViTac2024_Paper_28.pdf">Daniel Palenicek, Theo Gruner, Tim Schneider, Alina Bo ̈hm, Janis Lenz, Inga Pfenning, Eric Kra ̈mer† and Jan Peters. "Learning Tactile Insertion in the Real World" </a></li>
  <li><a href="content/ViTac2024_Paper_29.pdf">Guillaume Duret, Florence Zara, Jan Peters and Liming Chen. "Toward synthetic data generation for robotic tactile manipulations" </a></li>
</ol>

**Organisers:** <br>
[Shan Luo](https://shanluo.github.io/), King's College London <br>
[Nathan Lepora](www.lepora.com), University of Bristol <br>
[Wenzhen Yuan](https://cs.illinois.edu/about/people/adjunct-faculty/yuanwz), University of Illinois Urbana-Champaign <br>
[Rui Chen](https://callmeray.github.io/homepage/Home.html), Tsinghua University <br>
[Hao Su](https://cseweb.ucsd.edu/~haosu/ ), University of California San Diego <br>
[Kaspar Althoefer](http://www.eecs.qmul.ac.uk/profiles/althoeferkaspar.html), Queen Mary University of London <br>
[Gordon Cheng](https://www.professoren.tum.de/en/cheng-gordon), Technische Universität München <br>

**Invited speakers:** <br>
[Antonio Bicchi](https://www.iit.it/people-details/-/people/antonio-bicchi), Italian Institute of Technology and the University of Pisa, Italy <br>
[Mark Cutkosky](https://profiles.stanford.edu/mark-cutkosky), Stanford University, USA <br>
[Roberto Calandra](https://lasr.org/), Technische Universität Dresden, Germany <br>
[Huaping Liu](https://sites.google.com/site/thuliuhuaping), Tsinghua University, China <br>
[Perla Maiolino](https://eng.ox.ac.uk/people/perla-maiolino/), University of Oxford, UK <br>
[Ravinder Dahiya](https://coe.northeastern.edu/people/dahiya-ravinder/), Northeast University, USA <br>
[Van Anh Ho](https://fp.jaist.ac.jp/public/Default2.aspx?id=669&l=1), JAIST, Japan <br>
[Maria Bauza Villalonga](https://web.mit.edu/bauza/www/), DeepMind <br>

**Key dates** <br>
Posters and live demonstrations will be selected from a call for extended abstracts, reviewed by the organisers. The best posters will be invited to talk at the workshop. All submissions will be reviewed using a single-blind review process. Accepted contributions will be presented during the workshop as posters. <br>

Expected contributions should be submitted in the form of extended abstracts (max 2 pages + references) in IEEE Conference paper format. Submissions must be sent in pdf (<5MB), to: shan.luo@kcl.ac.uk, indicating [ICRA 2024 Workshop] in the email subject. <br>
Submission Deadline: <s>15th March</s> 30th March, 2024 <br>
Notification of acceptance: <s>30th March</s> 15th April, 2024 <br>
Camera-ready deadline: <s>15th April</s> 30th April, 2024 <br>
Workshop day: 17th May 2024 <br>

The accepted papers will be invited to publish their accepted extended abstracts in Springer Nature conference proceedings series Lecture Notes in Computer Science (LNCS). <br>

**Topics of Interest:** <br>
●  	Development of tactile sensors for robot tasks <br>
●  	Simulation of tactile sensors and Sim2Real learning <br>
●  	Visual-tactile sensing and perception <br>
●  	The control and design of robotic visuo-tactile systems for human-robot interaction <br>
●  	Interactive visuo-tactile perception in robot grasping and manipulation <br>
●  	Cognitive control of movement and sensory-motor representations with vision and touch <br>
●  	Bio-inspired approaches and cognitive architectures for the visuo-tactile perception <br>
●    Computational methods for processing vision and touch data in robot learning <br>

We thank the support from the following IEEE RAS Technical Committees: <br>
-   Haptics <br>
-   Cognitive Robotics <br>
-   Human-Robot Interaction & Coordination <br>

<!-- 
{% include toc.html %}

------

{% include template/credits.html %} -->
